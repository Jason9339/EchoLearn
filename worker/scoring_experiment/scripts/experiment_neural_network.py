"""
å¯¦é©—äºŒï¼šç¥ç¶“ç¶²è·¯è©•åˆ†æ¨¡å‹

è¨­è¨ˆç°¡å–®æœ‰æ•ˆçš„ DL æ¨¡å‹ä¾†é æ¸¬ 1-5 æ˜Ÿè©•åˆ†
æ’é™¤ WER ä»¥æ”¯æ´å¤šèªè¨€ï¼ˆ7 å€‹ç‰¹å¾µï¼‰

æ¨¡å‹æ¶æ§‹ï¼š
1. MLP Regressor - ç°¡å–®å¤šå±¤æ„ŸçŸ¥æ©Ÿ
2. MLP Classifier - å°‡è©•åˆ†è¦–ç‚º 5 åˆ†é¡å•é¡Œ
3. Ordinal Regression - æœ‰åºåˆ†é¡ï¼ˆè€ƒæ…®æ˜Ÿç´šé †åºï¼‰

æ³¨æ„ï¼šæ­¤è…³æœ¬è¨­è¨ˆç”¨æ–¼ GPU æ©Ÿå™¨åŸ·è¡Œ
"""

import json
import os
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error
import warnings
warnings.filterwarnings('ignore')

# ============================================================
# é…ç½®ï¼šæ’é™¤ WERï¼Œæ”¯æ´å¤šèªè¨€
# ============================================================
FEATURE_COLUMNS = [
    'score_PER',        # Phoneme Error Rate (reverse)
    'score_PPG',        # Phoneme Posteriorgram
    'score_GOP',        # Goodness of Pronunciation
    'score_GPE_offset', # Pronunciation Evaluation
    'score_FFE',        # Formant Fluency
    'score_Energy',     # Energy Similarity
    'score_VDE'         # Voice Distance
]

REVERSE_FEATURES = ['score_PER']  # éœ€è¦åè½‰çš„ç‰¹å¾µï¼ˆerror rateï¼‰

NUM_FEATURES = len(FEATURE_COLUMNS)
NUM_CLASSES = 5  # 1-5 æ˜Ÿ


# ============================================================
# è³‡æ–™è¼‰å…¥
# ============================================================
def load_data(dataset_path, min_raters=1):
    """è¼‰å…¥è³‡æ–™ï¼ˆæ’é™¤ WERï¼‰"""
    print(f"è¼‰å…¥è³‡æ–™é›†...")
    print(f"ä½¿ç”¨ç‰¹å¾µ ({NUM_FEATURES} å€‹ï¼Œå·²æ’é™¤ WER)ï¼š")
    for col in FEATURE_COLUMNS:
        print(f"  - {col}")

    with open(dataset_path, 'r') as f:
        data = json.load(f)

    rated_data = [d for d in data if d.get('rating_count', 0) >= min_raters]

    X = []
    y = []

    for item in rated_data:
        features = []
        valid = True

        for col in FEATURE_COLUMNS:
            if col not in item or item[col] is None:
                valid = False
                break
            score = item[col]
            # åè½‰ error rate
            if col in REVERSE_FEATURES:
                score = 1 - score
            features.append(score)

        if valid and 'rating_avg' in item:
            X.append(features)
            y.append(item['rating_avg'])

    return np.array(X), np.array(y)


# ============================================================
# æ¨¡å‹ 1: MLP Regressor
# ============================================================
class MLPRegressor(nn.Module):
    """
    ç°¡å–®çš„å¤šå±¤æ„ŸçŸ¥æ©Ÿå›æ­¸æ¨¡å‹

    æ¶æ§‹ï¼š
    - Input: 7 features
    - Hidden 1: 32 neurons + ReLU + Dropout
    - Hidden 2: 16 neurons + ReLU + Dropout
    - Output: 1 (rating score)
    """
    def __init__(self, input_dim=NUM_FEATURES, hidden_dims=[32, 16], dropout=0.3):
        super(MLPRegressor, self).__init__()

        layers = []
        prev_dim = input_dim

        for hidden_dim in hidden_dims:
            layers.extend([
                nn.Linear(prev_dim, hidden_dim),
                nn.ReLU(),
                nn.BatchNorm1d(hidden_dim),
                nn.Dropout(dropout)
            ])
            prev_dim = hidden_dim

        layers.append(nn.Linear(prev_dim, 1))

        self.network = nn.Sequential(*layers)

    def forward(self, x):
        return self.network(x).squeeze(-1)


# ============================================================
# æ¨¡å‹ 2: MLP Classifier (5 é¡åˆ¥)
# ============================================================
class MLPClassifier(nn.Module):
    """
    å¤šå±¤æ„ŸçŸ¥æ©Ÿåˆ†é¡æ¨¡å‹

    å°‡ 1-5 æ˜Ÿè¦–ç‚º 5 å€‹é¡åˆ¥
    è¼¸å‡º softmax æ©Ÿç‡åˆ†ä½ˆ
    """
    def __init__(self, input_dim=NUM_FEATURES, hidden_dims=[64, 32], dropout=0.3):
        super(MLPClassifier, self).__init__()

        layers = []
        prev_dim = input_dim

        for hidden_dim in hidden_dims:
            layers.extend([
                nn.Linear(prev_dim, hidden_dim),
                nn.ReLU(),
                nn.BatchNorm1d(hidden_dim),
                nn.Dropout(dropout)
            ])
            prev_dim = hidden_dim

        layers.append(nn.Linear(prev_dim, NUM_CLASSES))

        self.network = nn.Sequential(*layers)

    def forward(self, x):
        return self.network(x)

    def predict_star(self, x):
        """é æ¸¬æ˜Ÿç´š (1-5)"""
        logits = self.forward(x)
        return torch.argmax(logits, dim=1) + 1  # é¡åˆ¥ 0-4 â†’ æ˜Ÿç´š 1-5


# ============================================================
# æ¨¡å‹ 3: Ordinal Regression (æœ‰åºåˆ†é¡)
# ============================================================
class OrdinalRegressor(nn.Module):
    """
    æœ‰åºå›æ­¸æ¨¡å‹

    è€ƒæ…®æ˜Ÿç´šçš„é †åºæ€§ï¼š1 < 2 < 3 < 4 < 5
    ä½¿ç”¨ç´¯ç©æ©Ÿç‡å»ºæ¨¡ï¼šP(Y > k) for k = 1,2,3,4

    å„ªé»ï¼š
    - è€ƒæ…®åˆ° 3 æ˜Ÿæ¯” 4 æ˜Ÿæ›´æ¥è¿‘ 2 æ˜Ÿ
    - é æ¸¬æ›´å¹³æ»‘
    """
    def __init__(self, input_dim=NUM_FEATURES, hidden_dims=[32, 16], dropout=0.3):
        super(OrdinalRegressor, self).__init__()

        layers = []
        prev_dim = input_dim

        for hidden_dim in hidden_dims:
            layers.extend([
                nn.Linear(prev_dim, hidden_dim),
                nn.ReLU(),
                nn.BatchNorm1d(hidden_dim),
                nn.Dropout(dropout)
            ])
            prev_dim = hidden_dim

        # è¼¸å‡º 4 å€‹ç´¯ç©æ©Ÿç‡é–¾å€¼
        layers.append(nn.Linear(prev_dim, NUM_CLASSES - 1))

        self.network = nn.Sequential(*layers)

    def forward(self, x):
        # è¼¸å‡ºç´¯ç© logits
        return self.network(x)

    def predict_proba(self, x):
        """è¨ˆç®—æ¯å€‹æ˜Ÿç´šçš„æ©Ÿç‡"""
        cum_logits = self.forward(x)
        cum_probs = torch.sigmoid(cum_logits)

        # P(Y = k) = P(Y > k-1) - P(Y > k)
        probs = torch.zeros(x.size(0), NUM_CLASSES, device=x.device)
        probs[:, 0] = 1 - cum_probs[:, 0]

        for k in range(1, NUM_CLASSES - 1):
            probs[:, k] = cum_probs[:, k-1] - cum_probs[:, k]

        probs[:, -1] = cum_probs[:, -1]
        return probs

    def predict_star(self, x):
        """é æ¸¬æ˜Ÿç´š (1-5)"""
        probs = self.predict_proba(x)
        return torch.argmax(probs, dim=1) + 1

    def predict_expected(self, x):
        """è¨ˆç®—æœŸæœ›å€¼ï¼ˆåŠ æ¬Šå¹³å‡ï¼‰"""
        probs = self.predict_proba(x)
        stars = torch.arange(1, NUM_CLASSES + 1, dtype=torch.float32, device=x.device)
        return torch.sum(probs * stars, dim=1)


# ============================================================
# è¨“ç·´å‡½æ•¸
# ============================================================
def train_regressor(model, train_loader, val_loader, epochs=100, lr=0.001, device='cpu'):
    """è¨“ç·´å›æ­¸æ¨¡å‹"""
    model = model.to(device)
    criterion = nn.MSELoss()
    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=10, factor=0.5)

    best_val_loss = float('inf')
    best_model_state = None
    patience_counter = 0

    # æª¢æŸ¥æ˜¯å¦æ˜¯ Ordinal Regressorï¼ˆéœ€è¦ç‰¹æ®Šè™•ç†ï¼‰
    is_ordinal = isinstance(model, OrdinalRegressor)

    for epoch in range(epochs):
        # Training
        model.train()
        train_loss = 0
        for X_batch, y_batch in train_loader:
            X_batch, y_batch = X_batch.to(device), y_batch.to(device)

            optimizer.zero_grad()

            if is_ordinal:
                # Ordinal: ä½¿ç”¨æœŸæœ›å€¼é æ¸¬
                outputs = model.predict_expected(X_batch)
            else:
                outputs = model(X_batch)

            loss = criterion(outputs, y_batch)
            loss.backward()
            optimizer.step()

            train_loss += loss.item()

        train_loss /= len(train_loader)

        # Validation
        model.eval()
        val_loss = 0
        with torch.no_grad():
            for X_batch, y_batch in val_loader:
                X_batch, y_batch = X_batch.to(device), y_batch.to(device)

                if is_ordinal:
                    outputs = model.predict_expected(X_batch)
                else:
                    outputs = model(X_batch)

                val_loss += criterion(outputs, y_batch).item()

        val_loss /= len(val_loader)
        scheduler.step(val_loss)

        # Early stopping
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            best_model_state = model.state_dict().copy()
            patience_counter = 0
        else:
            patience_counter += 1

        if patience_counter >= 20:
            print(f"Early stopping at epoch {epoch+1}")
            break

        if (epoch + 1) % 20 == 0:
            print(f"Epoch {epoch+1}: Train Loss = {train_loss:.4f}, Val Loss = {val_loss:.4f}")

    model.load_state_dict(best_model_state)
    return model


def train_classifier(model, train_loader, val_loader, epochs=100, lr=0.001, device='cpu'):
    """è¨“ç·´åˆ†é¡æ¨¡å‹"""
    model = model.to(device)
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=10, factor=0.5)

    best_val_acc = 0
    best_model_state = None

    for epoch in range(epochs):
        # Training
        model.train()
        for X_batch, y_batch in train_loader:
            X_batch, y_batch = X_batch.to(device), y_batch.to(device)
            y_class = (y_batch.round() - 1).long().clamp(0, 4)  # 1-5 â†’ 0-4

            optimizer.zero_grad()
            outputs = model(X_batch)
            loss = criterion(outputs, y_class)
            loss.backward()
            optimizer.step()

        # Validation
        model.eval()
        correct = 0
        total = 0
        with torch.no_grad():
            for X_batch, y_batch in val_loader:
                X_batch, y_batch = X_batch.to(device), y_batch.to(device)
                y_class = (y_batch.round() - 1).long().clamp(0, 4)

                outputs = model(X_batch)
                _, predicted = torch.max(outputs, 1)
                total += y_batch.size(0)
                correct += (predicted == y_class).sum().item()

        val_acc = correct / total

        if val_acc > best_val_acc:
            best_val_acc = val_acc
            best_model_state = model.state_dict().copy()

        if (epoch + 1) % 20 == 0:
            print(f"Epoch {epoch+1}: Val Accuracy = {val_acc*100:.2f}%")

    model.load_state_dict(best_model_state)
    return model


# ============================================================
# è©•ä¼°å‡½æ•¸
# ============================================================
def evaluate_model(y_true, y_pred, model_name):
    """è©•ä¼°æ¨¡å‹è¡¨ç¾"""
    y_pred = np.clip(y_pred, 1, 5)

    rmse = np.sqrt(mean_squared_error(y_true, y_pred))
    mae = mean_absolute_error(y_true, y_pred)

    # æ•´æ•¸æ˜Ÿç´šæº–ç¢ºç‡
    y_true_int = np.round(y_true).astype(int)
    y_pred_int = np.round(y_pred).astype(int)
    exact_acc = np.mean(y_true_int == y_pred_int) * 100

    # ç›¸é„°æ˜Ÿç´šæº–ç¢ºç‡ (Â±1 æ˜Ÿ)
    adjacent_acc = np.mean(np.abs(y_true_int - y_pred_int) <= 1) * 100

    print(f"\n{model_name}:")
    print(f"  RMSE:             {rmse:.4f} æ˜Ÿ")
    print(f"  MAE:              {mae:.4f} æ˜Ÿ")
    print(f"  ç²¾ç¢ºåŒ¹é…æº–ç¢ºç‡:    {exact_acc:.2f}%")
    print(f"  ç›¸é„°æ˜Ÿç´šæº–ç¢ºç‡:    {adjacent_acc:.2f}% â† ä¸»è¦æŒ‡æ¨™")

    return {
        'rmse': float(rmse),
        'mae': float(mae),
        'exact_accuracy': float(exact_acc),
        'adjacent_accuracy': float(adjacent_acc)
    }


# ============================================================
# ä¸»ç¨‹å¼
# ============================================================
def main():
    print("="*80)
    print("å¯¦é©—äºŒï¼šç¥ç¶“ç¶²è·¯è©•åˆ†æ¨¡å‹")
    print("="*80)
    print("\næ³¨æ„ï¼šå·²æ’é™¤ WER ä»¥æ”¯æ´å¤šèªè¨€")

    # è¨­å‚™æª¢æ¸¬
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    print(f"ä½¿ç”¨è¨­å‚™: {device}")

    if device == 'cpu':
        print("âš ï¸  æœªæª¢æ¸¬åˆ° GPUï¼Œå°‡ä½¿ç”¨ CPU è¨“ç·´ï¼ˆè¼ƒæ…¢ï¼‰")

    # è·¯å¾‘è¨­å®š
    script_dir = os.path.dirname(os.path.abspath(__file__))
    base_dir = os.path.dirname(script_dir)
    worker_dir = os.path.dirname(base_dir)
    dataset_path = os.path.join(worker_dir, 'temp/dataset/dataset.json')

    # è¼‰å…¥è³‡æ–™
    X, y = load_data(dataset_path, min_raters=1)
    print(f"\næ¨£æœ¬æ•¸: {len(X)}")

    # åˆ†å‰²è³‡æ–™
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42
    )
    X_train, X_val, y_train, y_val = train_test_split(
        X_train, y_train, test_size=0.2, random_state=42
    )

    print(f"è¨“ç·´é›†: {len(X_train)}, é©—è­‰é›†: {len(X_val)}, æ¸¬è©¦é›†: {len(X_test)}")

    # ç‰¹å¾µæ¨™æº–åŒ–
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_val_scaled = scaler.transform(X_val)
    X_test_scaled = scaler.transform(X_test)

    # è½‰æ›ç‚º PyTorch tensors
    train_dataset = TensorDataset(
        torch.FloatTensor(X_train_scaled),
        torch.FloatTensor(y_train)
    )
    val_dataset = TensorDataset(
        torch.FloatTensor(X_val_scaled),
        torch.FloatTensor(y_val)
    )
    test_dataset = TensorDataset(
        torch.FloatTensor(X_test_scaled),
        torch.FloatTensor(y_test)
    )

    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=32)
    test_loader = DataLoader(test_dataset, batch_size=32)

    results = {}

    # ============================================================
    # æ¨¡å‹ 1: MLP Regressor
    # ============================================================
    print("\n" + "="*80)
    print("è¨“ç·´æ¨¡å‹ 1: MLP Regressor")
    print("="*80)

    mlp_reg = MLPRegressor(input_dim=NUM_FEATURES, hidden_dims=[32, 16], dropout=0.3)
    print(f"æ¨¡å‹æ¶æ§‹:\n{mlp_reg}")

    mlp_reg = train_regressor(mlp_reg, train_loader, val_loader,
                               epochs=100, lr=0.001, device=device)

    # è©•ä¼°
    mlp_reg.eval()
    with torch.no_grad():
        X_test_tensor = torch.FloatTensor(X_test_scaled).to(device)
        y_pred_reg = mlp_reg(X_test_tensor).cpu().numpy()

    results['mlp_regressor'] = evaluate_model(y_test, y_pred_reg, "MLP Regressor")

    # ============================================================
    # æ¨¡å‹ 2: MLP Classifier
    # ============================================================
    print("\n" + "="*80)
    print("è¨“ç·´æ¨¡å‹ 2: MLP Classifier (5 é¡åˆ¥)")
    print("="*80)

    mlp_cls = MLPClassifier(input_dim=NUM_FEATURES, hidden_dims=[64, 32], dropout=0.3)
    print(f"æ¨¡å‹æ¶æ§‹:\n{mlp_cls}")

    mlp_cls = train_classifier(mlp_cls, train_loader, val_loader,
                                epochs=100, lr=0.001, device=device)

    # è©•ä¼°
    mlp_cls.eval()
    with torch.no_grad():
        X_test_tensor = torch.FloatTensor(X_test_scaled).to(device)
        y_pred_cls = mlp_cls.predict_star(X_test_tensor).cpu().numpy().astype(float)

    results['mlp_classifier'] = evaluate_model(y_test, y_pred_cls, "MLP Classifier")

    # ============================================================
    # æ¨¡å‹ 3: Ordinal Regressor
    # ============================================================
    print("\n" + "="*80)
    print("è¨“ç·´æ¨¡å‹ 3: Ordinal Regressor")
    print("="*80)

    ord_reg = OrdinalRegressor(input_dim=NUM_FEATURES, hidden_dims=[32, 16], dropout=0.3)
    print(f"æ¨¡å‹æ¶æ§‹:\n{ord_reg}")

    ord_reg = train_regressor(ord_reg, train_loader, val_loader,
                               epochs=100, lr=0.001, device=device)

    # è©•ä¼°ï¼ˆä½¿ç”¨æœŸæœ›å€¼ï¼‰
    ord_reg.eval()
    with torch.no_grad():
        X_test_tensor = torch.FloatTensor(X_test_scaled).to(device)
        y_pred_ord = ord_reg.predict_expected(X_test_tensor).cpu().numpy()

    results['ordinal_regressor'] = evaluate_model(y_test, y_pred_ord, "Ordinal Regressor")

    # ============================================================
    # çµæœæ¯”è¼ƒ
    # ============================================================
    print("\n" + "="*80)
    print("ğŸ“Š æ¨¡å‹æ¯”è¼ƒç¸½çµ")
    print("="*80)

    print(f"\n{'æ¨¡å‹':<20} {'RMSE':<10} {'MAE':<10} {'ç²¾ç¢ºåŒ¹é…':<12} {'ç›¸é„°æº–ç¢ºç‡':<12}")
    print("-" * 65)

    for model_name, metrics in results.items():
        print(f"{model_name:<20} {metrics['rmse']:<10.4f} {metrics['mae']:<10.4f} "
              f"{metrics['exact_accuracy']:<12.2f} {metrics['adjacent_accuracy']:<12.2f}")

    # æ‰¾æœ€ä½³æ¨¡å‹
    best_model = max(results.items(), key=lambda x: x[1]['adjacent_accuracy'])
    print(f"\nğŸ† æœ€ä½³æ¨¡å‹: {best_model[0]} (ç›¸é„°æº–ç¢ºç‡: {best_model[1]['adjacent_accuracy']:.2f}%)")

    # ============================================================
    # å„²å­˜çµæœ
    # ============================================================
    output_dir = os.path.join(base_dir, 'results')
    os.makedirs(output_dir, exist_ok=True)

    output_path = os.path.join(output_dir, 'neural_network_results.json')
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump({
            'features': FEATURE_COLUMNS,
            'excluded_features': ['score_WER'],
            'num_features': NUM_FEATURES,
            'device': device,
            'results': results,
            'best_model': best_model[0]
        }, f, indent=2, ensure_ascii=False)

    print(f"\nâœ… çµæœå·²å„²å­˜: {output_path}")

    # å„²å­˜æ¨¡å‹ï¼ˆå¦‚æœéœ€è¦ï¼‰
    model_dir = os.path.join(base_dir, 'models')
    os.makedirs(model_dir, exist_ok=True)

    torch.save({
        'mlp_regressor': mlp_reg.state_dict(),
        'mlp_classifier': mlp_cls.state_dict(),
        'ordinal_regressor': ord_reg.state_dict(),
        'scaler_mean': scaler.mean_,
        'scaler_scale': scaler.scale_,
        'feature_columns': FEATURE_COLUMNS
    }, os.path.join(model_dir, 'neural_network_models.pth'))

    print(f"âœ… æ¨¡å‹å·²å„²å­˜: {os.path.join(model_dir, 'neural_network_models.pth')}")

    print("\n" + "="*80)
    print("âœ… å¯¦é©—äºŒå®Œæˆï¼")
    print("="*80)

    return results


if __name__ == '__main__':
    main()
