"""
å¯¦é©—ä¸€ï¼šå®¹å¿å€é–“æº–ç¢ºç‡è©•ä¼°

ç›®æ¨™ï¼šæ‰¾å‡ºæœ€ä½³çš„å®¹å¿å€é–“ n%ï¼Œè®“æº–ç¢ºç‡æ›´æœ‰è§£é‡‹æ€§

è©•ä¼°æ–¹å¼ï¼š
- å°æ¯å€‹ç‰¹å¾µè¨­å®šå®¹å¿å€é–“
- é æ¸¬å€¼èˆ‡çœŸå¯¦å€¼çš„å·®ç•°åœ¨ Â±n% ç¯„åœå…§ç®—ã€Œæ­£ç¢ºã€
- æ¸¬è©¦ä¸åŒ n å€¼ (0-100%) æ‰¾å‡ºæœ€ä½³å¹³è¡¡é»

æ³¨æ„ï¼šæ’é™¤ WER ä»¥æ”¯æ´å¤šèªè¨€
"""

import json
import os
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error
import matplotlib.pyplot as plt

# æ’é™¤ WERï¼Œæ”¯æ´å¤šèªè¨€
METRIC_INFO = {
    'score_PER': {'reverse': True, 'name': 'Phoneme Error Rate'},
    'score_PPG': {'reverse': False, 'name': 'Phoneme Posteriorgram'},
    # 'score_WER': {'reverse': True},  # æ’é™¤ - é™å®šè‹±æ–‡
    'score_GOP': {'reverse': False, 'name': 'Goodness of Pronunciation'},
    'score_GPE_offset': {'reverse': False, 'name': 'Pronunciation Evaluation'},
    'score_FFE': {'reverse': False, 'name': 'Formant Fluency'},
    'score_Energy': {'reverse': False, 'name': 'Energy Similarity'},
    'score_VDE': {'reverse': False, 'name': 'Voice Distance'}
}

SCORE_COLUMNS = list(METRIC_INFO.keys())


def load_data(dataset_path, min_raters=1):
    """è¼‰å…¥è³‡æ–™ï¼ˆæ’é™¤ WERï¼‰"""
    print(f"è¼‰å…¥è³‡æ–™é›†...")
    with open(dataset_path, 'r') as f:
        data = json.load(f)

    rated_data = [d for d in data if d.get('rating_count', 0) >= min_raters]

    X = []
    y = []

    for item in rated_data:
        features = []
        valid = True

        for col in SCORE_COLUMNS:
            if col not in item or item[col] is None:
                valid = False
                break
            score = item[col]
            if METRIC_INFO[col]['reverse']:
                score = 1 - score
            features.append(score)

        if valid and 'rating_avg' in item:
            X.append(features)
            y.append(item['rating_avg'])

    return np.array(X), np.array(y)


def tolerance_accuracy(y_true, y_pred, tolerance_percent):
    """
    è¨ˆç®—å®¹å¿å€é–“æº–ç¢ºç‡

    å®¹å¿å€é–“å®šç¾©ï¼š
    - å°æ–¼ 1-5 æ˜Ÿè©•åˆ†ï¼Œç¸½ç¯„åœæ˜¯ 4 æ˜Ÿ
    - tolerance_percent% çš„å®¹å¿ç¯„åœ = 4 * (tolerance_percent / 100)

    ä¾‹å¦‚ï¼š20% å®¹å¿ç¯„åœ = 4 * 0.2 = 0.8 æ˜Ÿ
    """
    range_size = 4.0  # 1-5 æ˜Ÿçš„ç¯„åœ
    tolerance = range_size * (tolerance_percent / 100)

    errors = np.abs(y_pred - y_true)
    accurate = errors <= tolerance

    return np.mean(accurate) * 100


def star_based_accuracy(y_true, y_pred, tolerance_stars):
    """
    ä»¥æ˜Ÿç´šç‚ºå–®ä½çš„å®¹å¿æº–ç¢ºç‡

    ä¾‹å¦‚ï¼štolerance_stars=0.5 è¡¨ç¤ºèª¤å·®åœ¨ Â±0.5 æ˜Ÿå…§ç®—æ­£ç¢º
    """
    errors = np.abs(y_pred - y_true)
    accurate = errors <= tolerance_stars
    return np.mean(accurate) * 100


def integer_star_accuracy(y_true, y_pred):
    """
    æ•´æ•¸æ˜Ÿç´šæº–ç¢ºç‡ï¼ˆå››æ¨äº”å…¥å¾Œå®Œå…¨ç›¸åŒï¼‰
    """
    y_true_int = np.round(y_true).astype(int)
    y_pred_int = np.round(y_pred).astype(int)
    return np.mean(y_true_int == y_pred_int) * 100


def adjacent_star_accuracy(y_true, y_pred):
    """
    ç›¸é„°æ˜Ÿç´šæº–ç¢ºç‡ï¼ˆå…è¨± Â±1 æ˜Ÿèª¤å·®ï¼‰

    é€™æ˜¯ä¸€å€‹å¾ˆå¥½çš„æŒ‡æ¨™ï¼š
    - å¦‚æœçœŸå¯¦æ˜¯ 4 æ˜Ÿï¼Œé æ¸¬ 3/4/5 æ˜Ÿéƒ½ç®—æ­£ç¢º
    - å°ç”¨æˆ¶ä¾†èªªï¼ŒÂ±1 æ˜Ÿçš„èª¤å·®æ˜¯å¯æ¥å—çš„
    """
    y_true_int = np.round(y_true).astype(int)
    y_pred_int = np.round(y_pred).astype(int)
    diff = np.abs(y_true_int - y_pred_int)
    return np.mean(diff <= 1) * 100


def run_experiment(dataset_path):
    """
    åŸ·è¡Œå®¹å¿å€é–“å¯¦é©—
    """
    print("="*80)
    print("å¯¦é©—ä¸€ï¼šå®¹å¿å€é–“æº–ç¢ºç‡è©•ä¼°")
    print("="*80)
    print("\nç›®æ¨™ï¼šæ‰¾å‡ºæœ€ä½³çš„è©•ä¼°æŒ‡æ¨™ï¼Œè®“çµæœæ›´æœ‰è§£é‡‹æ€§")
    print("æ³¨æ„ï¼šå·²æ’é™¤ WER æŒ‡æ¨™ä»¥æ”¯æ´å¤šèªè¨€")
    print(f"ä½¿ç”¨ç‰¹å¾µï¼š{SCORE_COLUMNS}")

    # è¼‰å…¥è³‡æ–™
    X, y = load_data(dataset_path)
    print(f"\næ¨£æœ¬æ•¸: {len(X)}")
    print(f"ç‰¹å¾µæ•¸: {len(SCORE_COLUMNS)} (æ’é™¤ WER)")

    # åˆ†å‰²è³‡æ–™
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42
    )

    # è¨“ç·´æ¨¡å‹
    print("\nè¨“ç·´ Random Forest...")
    model = RandomForestRegressor(
        n_estimators=100,
        max_depth=10,
        min_samples_split=5,
        min_samples_leaf=2,
        random_state=42,
        n_jobs=-1
    )
    model.fit(X_train, y_train)

    # é æ¸¬
    y_pred = model.predict(X_test)
    y_pred = np.clip(y_pred, 1, 5)

    # åŸºç¤æŒ‡æ¨™
    rmse = np.sqrt(mean_squared_error(y_test, y_pred))
    mae = np.mean(np.abs(y_test - y_pred))

    print("\n" + "="*80)
    print("åŸºç¤æŒ‡æ¨™")
    print("="*80)
    print(f"RMSE: {rmse:.4f} æ˜Ÿ")
    print(f"MAE:  {mae:.4f} æ˜Ÿ")

    # ============================================================
    # å¯¦é©— 1: ä¸åŒå®¹å¿ç™¾åˆ†æ¯”
    # ============================================================
    print("\n" + "="*80)
    print("å¯¦é©— 1: å®¹å¿ç™¾åˆ†æ¯” (n%) åˆ†æ")
    print("="*80)
    print("\nå®¹å¿ç¯„åœ = 4 æ˜Ÿ Ã— n%")
    print("ä¾‹å¦‚ï¼š20% = Â±0.8 æ˜Ÿ")

    tolerance_percents = [5, 10, 15, 20, 25, 30, 40, 50]
    results_percent = []

    print(f"\n{'å®¹å¿%':<10} {'å®¹å¿ç¯„åœ':<12} {'æº–ç¢ºç‡':<10}")
    print("-" * 35)

    for pct in tolerance_percents:
        tolerance_range = 4.0 * (pct / 100)
        acc = tolerance_accuracy(y_test, y_pred, pct)
        results_percent.append({'percent': pct, 'range': tolerance_range, 'accuracy': acc})
        print(f"{pct}%        Â±{tolerance_range:.2f} æ˜Ÿ      {acc:.2f}%")

    # ============================================================
    # å¯¦é©— 2: ä»¥æ˜Ÿç´šç‚ºå–®ä½çš„å®¹å¿
    # ============================================================
    print("\n" + "="*80)
    print("å¯¦é©— 2: æ˜Ÿç´šå®¹å¿åˆ†æ")
    print("="*80)

    star_tolerances = [0.25, 0.5, 0.75, 1.0, 1.5, 2.0]
    results_star = []

    print(f"\n{'å®¹å¿æ˜Ÿç´š':<12} {'æº–ç¢ºç‡':<10} {'è§£é‡‹':<30}")
    print("-" * 55)

    for tol in star_tolerances:
        acc = star_based_accuracy(y_test, y_pred, tol)
        results_star.append({'tolerance': tol, 'accuracy': acc})

        if tol == 0.5:
            desc = "åŠæ˜Ÿç²¾æº–"
        elif tol == 1.0:
            desc = "Â±1 æ˜Ÿå¯æ¥å—èª¤å·®"
        elif tol == 0.25:
            desc = "æ¥µé«˜ç²¾æº–åº¦"
        else:
            desc = ""

        print(f"Â±{tol} æ˜Ÿ       {acc:.2f}%     {desc}")

    # ============================================================
    # å¯¦é©— 3: æ•´æ•¸æ˜Ÿç´šæº–ç¢ºç‡
    # ============================================================
    print("\n" + "="*80)
    print("å¯¦é©— 3: æ•´æ•¸æ˜Ÿç´šæº–ç¢ºç‡ï¼ˆå ±å‘Šç”¨ï¼‰")
    print("="*80)

    exact_acc = integer_star_accuracy(y_test, y_pred)
    adjacent_acc = adjacent_star_accuracy(y_test, y_pred)

    print(f"\n1. ç²¾ç¢ºåŒ¹é…æº–ç¢ºç‡ (Exact Match):     {exact_acc:.2f}%")
    print(f"   â†’ é æ¸¬æ˜Ÿç´šèˆ‡çœŸå¯¦æ˜Ÿç´šå®Œå…¨ç›¸åŒ")

    print(f"\n2. ç›¸é„°æ˜Ÿç´šæº–ç¢ºç‡ (Adjacent Match):  {adjacent_acc:.2f}%")
    print(f"   â†’ é æ¸¬æ˜Ÿç´šèˆ‡çœŸå¯¦æ˜Ÿç´šç›¸å·® â‰¤1 æ˜Ÿ")
    print(f"   â†’ é€™æ˜¯æœ€é©åˆå ±å‘Šçš„æŒ‡æ¨™ï¼")

    # ============================================================
    # å»ºè­°çš„å ±å‘ŠæŒ‡æ¨™
    # ============================================================
    print("\n" + "="*80)
    print("ğŸ“Š å»ºè­°çš„å ±å‘ŠæŒ‡æ¨™")
    print("="*80)

    print(f"""
æ¨è–¦ä½¿ç”¨ä»¥ä¸‹æŒ‡æ¨™ä¾†å ±å‘Šï¼š

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  æŒ‡æ¨™åç¨±                    æ•¸å€¼          è§£é‡‹              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  ç›¸é„°æ˜Ÿç´šæº–ç¢ºç‡ (Â±1 æ˜Ÿ)      {adjacent_acc:.1f}%        â† ä¸»è¦æŒ‡æ¨™    â”‚
â”‚  ç²¾ç¢ºåŒ¹é…æº–ç¢ºç‡              {exact_acc:.1f}%                        â”‚
â”‚  åŠæ˜Ÿæº–ç¢ºç‡ (Â±0.5 æ˜Ÿ)        {star_based_accuracy(y_test, y_pred, 0.5):.1f}%                        â”‚
â”‚  RMSE                        {rmse:.2f} æ˜Ÿ                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

å ±å‘Šç¯„ä¾‹ï¼š
  "æˆ‘å€‘çš„è©•åˆ†ç³»çµ±é”åˆ° {adjacent_acc:.1f}% çš„ç›¸é„°æ˜Ÿç´šæº–ç¢ºç‡ï¼Œ
   è¡¨ç¤ºé æ¸¬çµæœèˆ‡äººå·¥è©•åˆ†çš„èª¤å·®åœ¨ Â±1 æ˜Ÿä»¥å…§ã€‚"
    """)

    # ============================================================
    # ç‰¹å¾µé‡è¦æ€§
    # ============================================================
    print("\n" + "="*80)
    print("ç‰¹å¾µé‡è¦æ€§ï¼ˆæ’é™¤ WERï¼‰")
    print("="*80)

    importances = model.feature_importances_
    for col, imp in sorted(zip(SCORE_COLUMNS, importances), key=lambda x: x[1], reverse=True):
        print(f"  {col:20s}: {imp:.4f}")

    # ============================================================
    # å„²å­˜çµæœ
    # ============================================================
    results = {
        'basic_metrics': {
            'rmse': float(rmse),
            'mae': float(mae)
        },
        'recommended_metrics': {
            'adjacent_star_accuracy': float(adjacent_acc),
            'exact_match_accuracy': float(exact_acc),
            'half_star_accuracy': float(star_based_accuracy(y_test, y_pred, 0.5))
        },
        'tolerance_percent_analysis': results_percent,
        'star_tolerance_analysis': [
            {'tolerance': r['tolerance'], 'accuracy': r['accuracy']}
            for r in results_star
        ],
        'feature_importance': {
            col: float(imp) for col, imp in zip(SCORE_COLUMNS, importances)
        },
        'excluded_features': ['score_WER'],
        'reason': 'WER excluded for multi-language support'
    }

    return results, model, X_test, y_test, y_pred


def plot_tolerance_curve(results, output_dir):
    """
    ç¹ªè£½å®¹å¿å€é–“æº–ç¢ºç‡æ›²ç·š
    """
    fig, axes = plt.subplots(1, 2, figsize=(14, 5))

    # åœ– 1: æ˜Ÿç´šå®¹å¿æº–ç¢ºç‡
    ax1 = axes[0]
    tolerances = [r['tolerance'] for r in results['star_tolerance_analysis']]
    accuracies = [r['accuracy'] for r in results['star_tolerance_analysis']]

    bars = ax1.bar(range(len(tolerances)), accuracies, color='#3498db', alpha=0.7, edgecolor='black')
    ax1.set_xticks(range(len(tolerances)))
    ax1.set_xticklabels([f'Â±{t}' for t in tolerances])
    ax1.set_xlabel('Tolerance (stars)', fontsize=12, fontweight='bold')
    ax1.set_ylabel('Accuracy (%)', fontsize=12, fontweight='bold')
    ax1.set_title('Accuracy by Star Tolerance', fontsize=14, fontweight='bold')
    ax1.set_ylim(0, 100)
    ax1.grid(True, alpha=0.3, axis='y')

    # æ¨™è¨˜ Â±1 æ˜Ÿï¼ˆæ¨è–¦ï¼‰
    for i, (tol, acc) in enumerate(zip(tolerances, accuracies)):
        color = 'red' if tol == 1.0 else 'black'
        weight = 'bold' if tol == 1.0 else 'normal'
        ax1.text(i, acc + 2, f'{acc:.1f}%', ha='center', va='bottom',
                fontsize=10, fontweight=weight, color=color)

    # åœ– 2: ç™¾åˆ†æ¯”å®¹å¿æº–ç¢ºç‡
    ax2 = axes[1]
    percents = [r['percent'] for r in results['tolerance_percent_analysis']]
    percent_accs = [r['accuracy'] for r in results['tolerance_percent_analysis']]

    ax2.plot(percents, percent_accs, 'o-', linewidth=2, markersize=8, color='#e74c3c')
    ax2.fill_between(percents, percent_accs, alpha=0.2, color='#e74c3c')
    ax2.set_xlabel('Tolerance (%)', fontsize=12, fontweight='bold')
    ax2.set_ylabel('Accuracy (%)', fontsize=12, fontweight='bold')
    ax2.set_title('Accuracy by Percentage Tolerance', fontsize=14, fontweight='bold')
    ax2.set_ylim(0, 100)
    ax2.grid(True, alpha=0.3)

    # æ¨™è¨˜ 25%ï¼ˆæ¨è–¦ï¼‰
    for pct, acc in zip(percents, percent_accs):
        if pct == 25:
            ax2.annotate(f'{acc:.1f}%\n(Â±1 star)',
                        xy=(pct, acc),
                        xytext=(pct + 5, acc - 10),
                        fontsize=10, fontweight='bold', color='red',
                        arrowprops=dict(arrowstyle='->', color='red'))

    plt.tight_layout()
    output_path = os.path.join(output_dir, 'tolerance_accuracy_curves.png')
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    print(f"\nâœ… åœ–è¡¨å·²å„²å­˜: {output_path}")
    plt.close()


if __name__ == '__main__':
    script_dir = os.path.dirname(os.path.abspath(__file__))
    base_dir = os.path.dirname(script_dir)
    worker_dir = os.path.dirname(base_dir)

    dataset_path = os.path.join(worker_dir, 'temp/dataset/dataset.json')
    output_dir = os.path.join(base_dir, 'results')

    results, model, X_test, y_test, y_pred = run_experiment(dataset_path)

    # ç¹ªè£½åœ–è¡¨
    plot_tolerance_curve(results, output_dir)

    # å„²å­˜çµæœ
    output_path = os.path.join(output_dir, 'tolerance_accuracy_results.json')
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    print(f"âœ… çµæœå·²å„²å­˜: {output_path}")

    print("\n" + "="*80)
    print("âœ… å¯¦é©—ä¸€å®Œæˆï¼")
    print("="*80)
