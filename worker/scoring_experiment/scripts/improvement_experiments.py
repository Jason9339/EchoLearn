"""
æ”¹é€²å¯¦é©—ï¼šæ¸¬è©¦å„ç¨®å„ªåŒ–æ–¹æ¡ˆçš„æ•ˆæœ

å„ªå…ˆç´šæ’åºï¼š
1. åªç”¨ â‰¥3 ä½è©•åˆ†è€…è³‡æ–™ï¼ˆè³‡æ–™å“è³ªï¼‰
2. XGBoost æ¨¡å‹ï¼ˆæ¨¡å‹å„ªåŒ–ï¼‰
3. è¶…åƒæ•¸èª¿æ ¡ï¼ˆFine-tuningï¼‰
"""

import json
import os
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import joblib

# å˜—è©¦å°å…¥ XGBoost
try:
    import xgboost as xgb
    XGBOOST_AVAILABLE = True
except ImportError:
    XGBOOST_AVAILABLE = False
    print("âš ï¸  XGBoost æœªå®‰è£ï¼Œå°‡è·³é XGBoost å¯¦é©—")

METRIC_INFO = {
    'score_PER': {'reverse': True},
    'score_PPG': {'reverse': False},
    'score_WER': {'reverse': True},
    'score_GOP': {'reverse': False},
    'score_GPE_offset': {'reverse': False},
    'score_FFE': {'reverse': False},
    'score_Energy': {'reverse': False},
    'score_VDE': {'reverse': False}
}

SCORE_COLUMNS = list(METRIC_INFO.keys())


def load_data(dataset_path, min_raters=1):
    """
    è¼‰å…¥è³‡æ–™ï¼Œå¯æŒ‡å®šæœ€å°‘è©•åˆ†äººæ•¸
    """
    print(f"è¼‰å…¥è³‡æ–™é›† (æœ€å°‘è©•åˆ†äººæ•¸: {min_raters})...")
    with open(dataset_path, 'r') as f:
        data = json.load(f)

    # ç¯©é¸æ¢ä»¶
    rated_data = [d for d in data
                  if d.get('rating_count', 0) >= min_raters]

    print(f"ç¸½æ¨£æœ¬æ•¸: {len(data)}")
    print(f"ç¬¦åˆæ¢ä»¶æ¨£æœ¬: {len(rated_data)}")

    # æå–ç‰¹å¾µå’Œæ¨™ç±¤
    X = []
    y = []

    for item in rated_data:
        features = []
        valid = True

        for col in SCORE_COLUMNS:
            if col not in item or item[col] is None:
                valid = False
                break

            score = item[col]
            if METRIC_INFO[col]['reverse']:
                score = 1 - score
            features.append(score)

        if valid and 'rating_avg' in item:
            X.append(features)
            y.append(item['rating_avg'])

    X = np.array(X)
    y = np.array(y)

    print(f"æœ‰æ•ˆè¨“ç·´æ¨£æœ¬: {len(X)}")
    return X, y


def evaluate_model(y_true, y_pred, model_name):
    """
    è©•ä¼°æ¨¡å‹è¡¨ç¾
    """
    y_pred_clipped = np.clip(y_pred, 1, 5)

    mae = mean_absolute_error(y_true, y_pred_clipped)
    rmse = np.sqrt(mean_squared_error(y_true, y_pred_clipped))
    r2 = r2_score(y_true, y_pred_clipped)

    # æ˜Ÿç´šæº–ç¢ºåº¦
    pred_int = np.round(y_pred_clipped)
    true_int = np.round(y_true)
    exact_acc = np.mean(pred_int == true_int)

    # èª¤å·®åˆ†ä½ˆ
    errors = np.abs(y_pred_clipped - y_true)
    within_1 = np.mean(errors <= 1.0)

    print(f"\n{model_name}:")
    print(f"  MAE:  {mae:.4f} æ˜Ÿ")
    print(f"  RMSE: {rmse:.4f} æ˜Ÿ")
    print(f"  RÂ²:   {r2:.4f}")
    print(f"  æ•´æ•¸æ˜Ÿç´šæº–ç¢ºåº¦: {exact_acc*100:.2f}%")
    print(f"  Â±1æ˜Ÿæº–ç¢ºåº¦:     {within_1*100:.2f}%")

    return {
        'mae': mae,
        'rmse': rmse,
        'r2': r2,
        'exact_accuracy': exact_acc,
        'within_1_star': within_1
    }


def experiment_1_data_quality(dataset_path):
    """
    å¯¦é©— 1: è³‡æ–™å“è³ªå½±éŸ¿
    æ¯”è¼ƒä¸åŒæœ€å°‘è©•åˆ†äººæ•¸çš„æ•ˆæœ
    """
    print("\n" + "="*80)
    print("å¯¦é©— 1: è³‡æ–™å“è³ªå½±éŸ¿ - åªä½¿ç”¨é«˜ä¿¡å¿ƒè©•åˆ†è³‡æ–™")
    print("="*80)

    results = {}

    for min_raters in [1, 2, 3]:
        print(f"\n{'='*80}")
        print(f"æœ€å°‘è©•åˆ†äººæ•¸: {min_raters}")
        print(f"{'='*80}")

        X, y = load_data(dataset_path, min_raters=min_raters)

        if len(X) < 100:
            print(f"âš ï¸  æ¨£æœ¬æ•¸å¤ªå°‘ ({len(X)})ï¼Œè·³é")
            continue

        # è¨“ç·´æ¸¬è©¦åˆ†å‰²
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42
        )

        # è¨“ç·´ Random Forest
        model = RandomForestRegressor(
            n_estimators=100,
            max_depth=10,
            min_samples_split=10,
            min_samples_leaf=5,
            random_state=42,
            n_jobs=-1
        )
        model.fit(X_train, y_train)

        # è©•ä¼°
        y_pred_test = model.predict(X_test)
        metrics = evaluate_model(y_test, y_pred_test, f"RF (â‰¥{min_raters} è©•åˆ†è€…)")

        results[f'min_raters_{min_raters}'] = {
            'sample_count': len(X),
            'metrics': metrics
        }

    return results


def experiment_2_xgboost(dataset_path):
    """
    å¯¦é©— 2: XGBoost vs Random Forest
    """
    print("\n" + "="*80)
    print("å¯¦é©— 2: XGBoost æ¨¡å‹å°æ¯”")
    print("="*80)

    if not XGBOOST_AVAILABLE:
        print("âš ï¸  XGBoost æœªå®‰è£ï¼Œè·³éæ­¤å¯¦é©—")
        return None

    X, y = load_data(dataset_path, min_raters=1)
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42
    )

    results = {}

    # Random Forest (baseline)
    print("\nè¨“ç·´ Random Forest (baseline)...")
    rf = RandomForestRegressor(
        n_estimators=100,
        max_depth=10,
        min_samples_split=10,
        min_samples_leaf=5,
        random_state=42,
        n_jobs=-1
    )
    rf.fit(X_train, y_train)
    y_pred_rf = rf.predict(X_test)
    results['random_forest'] = evaluate_model(y_test, y_pred_rf, "Random Forest")

    # XGBoost
    print("\nè¨“ç·´ XGBoost...")
    xgb_model = xgb.XGBRegressor(
        n_estimators=100,
        max_depth=6,
        learning_rate=0.1,
        subsample=0.8,
        colsample_bytree=0.8,
        random_state=42,
        n_jobs=-1
    )
    xgb_model.fit(X_train, y_train)
    y_pred_xgb = xgb_model.predict(X_test)
    results['xgboost'] = evaluate_model(y_test, y_pred_xgb, "XGBoost")

    # æ¯”è¼ƒ
    print("\n" + "="*80)
    print("æ¨¡å‹æ¯”è¼ƒ")
    print("="*80)
    mae_improvement = (results['random_forest']['mae'] - results['xgboost']['mae']) / results['random_forest']['mae'] * 100

    if results['xgboost']['mae'] < results['random_forest']['mae']:
        print(f"âœ… XGBoost å„ªæ–¼ RFï¼ŒMAE æ”¹é€² {mae_improvement:.2f}%")
        print(f"   RF MAE:  {results['random_forest']['mae']:.4f}")
        print(f"   XGB MAE: {results['xgboost']['mae']:.4f}")
        winner = 'xgboost'
    else:
        print(f"âš ï¸  RF ä»å„ªæ–¼ XGBoost")
        winner = 'random_forest'

    results['winner'] = winner
    return results


def experiment_3_hyperparameter_tuning(dataset_path):
    """
    å¯¦é©— 3: Random Forest è¶…åƒæ•¸èª¿æ ¡
    """
    print("\n" + "="*80)
    print("å¯¦é©— 3: Random Forest è¶…åƒæ•¸èª¿æ ¡")
    print("="*80)

    X, y = load_data(dataset_path, min_raters=1)
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42
    )

    results = {}

    # Baseline (ç•¶å‰åƒæ•¸)
    print("\nè¨“ç·´ Baseline...")
    baseline = RandomForestRegressor(
        n_estimators=100,
        max_depth=10,
        min_samples_split=10,
        min_samples_leaf=5,
        random_state=42,
        n_jobs=-1
    )
    baseline.fit(X_train, y_train)
    y_pred = baseline.predict(X_test)
    results['baseline'] = evaluate_model(y_test, y_pred, "Baseline RF")

    # èª¿æ•´ 1: å¢åŠ æ¨¹æ•¸é‡
    print("\nè¨“ç·´ æ›´å¤šæ¨¹ (200 æ£µ)...")
    more_trees = RandomForestRegressor(
        n_estimators=200,
        max_depth=10,
        min_samples_split=10,
        min_samples_leaf=5,
        random_state=42,
        n_jobs=-1
    )
    more_trees.fit(X_train, y_train)
    y_pred = more_trees.predict(X_test)
    results['more_trees'] = evaluate_model(y_test, y_pred, "RF (200 æ£µæ¨¹)")

    # èª¿æ•´ 2: å¢åŠ æ·±åº¦
    print("\nè¨“ç·´ æ›´æ·±çš„æ¨¹ (depth=15)...")
    deeper = RandomForestRegressor(
        n_estimators=100,
        max_depth=15,
        min_samples_split=10,
        min_samples_leaf=5,
        random_state=42,
        n_jobs=-1
    )
    deeper.fit(X_train, y_train)
    y_pred = deeper.predict(X_test)
    results['deeper'] = evaluate_model(y_test, y_pred, "RF (æ·±åº¦ 15)")

    # èª¿æ•´ 3: æ¸›å°‘æœ€å°æ¨£æœ¬æ•¸
    print("\nè¨“ç·´ æ›´éˆæ´»çš„æ¨¹ (min_samples=5)...")
    flexible = RandomForestRegressor(
        n_estimators=100,
        max_depth=10,
        min_samples_split=5,
        min_samples_leaf=2,
        random_state=42,
        n_jobs=-1
    )
    flexible.fit(X_train, y_train)
    y_pred = flexible.predict(X_test)
    results['flexible'] = evaluate_model(y_test, y_pred, "RF (æ›´éˆæ´»)")

    # æ‰¾å‡ºæœ€ä½³é…ç½®
    best_config = min(results.items(), key=lambda x: x[1]['mae'])
    print("\n" + "="*80)
    print(f"æœ€ä½³é…ç½®: {best_config[0]} (MAE: {best_config[1]['mae']:.4f})")
    print("="*80)

    results['best'] = best_config[0]
    return results


def main():
    """
    åŸ·è¡Œæ‰€æœ‰æ”¹é€²å¯¦é©—
    """
    script_dir = os.path.dirname(os.path.abspath(__file__))
    base_dir = os.path.dirname(script_dir)
    worker_dir = os.path.dirname(base_dir)

    dataset_path = os.path.join(worker_dir, 'temp/dataset/dataset.json')

    all_results = {}

    # å¯¦é©— 1: è³‡æ–™å“è³ª
    exp1_results = experiment_1_data_quality(dataset_path)
    all_results['experiment_1_data_quality'] = exp1_results

    # å¯¦é©— 2: XGBoost
    exp2_results = experiment_2_xgboost(dataset_path)
    if exp2_results:
        all_results['experiment_2_xgboost'] = exp2_results

    # å¯¦é©— 3: è¶…åƒæ•¸èª¿æ ¡
    exp3_results = experiment_3_hyperparameter_tuning(dataset_path)
    all_results['experiment_3_hyperparameters'] = exp3_results

    # ç¸½çµ
    print("\n" + "="*80)
    print("ğŸ“Š æ”¹é€²å¯¦é©—ç¸½çµ")
    print("="*80)

    # å¯¦é©— 1 ç¸½çµ
    print("\n1ï¸âƒ£  è³‡æ–™å“è³ªå½±éŸ¿:")
    if 'experiment_1_data_quality' in all_results:
        for key, result in all_results['experiment_1_data_quality'].items():
            min_raters = key.split('_')[-1]
            print(f"   â‰¥{min_raters} è©•åˆ†è€…: MAE = {result['metrics']['mae']:.4f} ({result['sample_count']} æ¨£æœ¬)")

    # å¯¦é©— 2 ç¸½çµ
    print("\n2ï¸âƒ£  XGBoost vs Random Forest:")
    if 'experiment_2_xgboost' in all_results:
        exp2 = all_results['experiment_2_xgboost']
        print(f"   Random Forest: MAE = {exp2['random_forest']['mae']:.4f}")
        print(f"   XGBoost:       MAE = {exp2['xgboost']['mae']:.4f}")
        print(f"   å‹è€…: {exp2['winner']}")
    else:
        print("   âš ï¸  XGBoost æœªå®‰è£ï¼Œç„¡æ³•æ¯”è¼ƒ")

    # å¯¦é©— 3 ç¸½çµ
    print("\n3ï¸âƒ£  è¶…åƒæ•¸èª¿æ ¡:")
    if 'experiment_3_hyperparameters' in all_results:
        exp3 = all_results['experiment_3_hyperparameters']
        baseline_mae = exp3['baseline']['mae']
        best_config = exp3['best']
        best_mae = exp3[best_config]['mae']
        improvement = (baseline_mae - best_mae) / baseline_mae * 100

        print(f"   Baseline:  MAE = {baseline_mae:.4f}")
        print(f"   æœ€ä½³é…ç½®:  {best_config}, MAE = {best_mae:.4f}")
        if improvement > 0:
            print(f"   âœ… æ”¹é€² {improvement:.2f}%")
        else:
            print(f"   âš ï¸  ç„¡æ˜é¡¯æ”¹é€²")

    # æœ€çµ‚å»ºè­°
    print("\n" + "="*80)
    print("ğŸ’¡ æœ€çµ‚å»ºè­°")
    print("="*80)

    recommendations = []

    # æª¢æŸ¥è³‡æ–™å“è³ªå½±éŸ¿
    if 'experiment_1_data_quality' in all_results:
        if 'min_raters_3' in all_results['experiment_1_data_quality']:
            mae_all = all_results['experiment_1_data_quality']['min_raters_1']['metrics']['mae']
            mae_3plus = all_results['experiment_1_data_quality']['min_raters_3']['metrics']['mae']

            if mae_3plus < mae_all * 0.95:  # æ”¹é€² 5% ä»¥ä¸Š
                recommendations.append({
                    'priority': 'high',
                    'action': 'åªä½¿ç”¨ â‰¥3 ä½è©•åˆ†è€…çš„è³‡æ–™',
                    'reason': f'MAE å¾ {mae_all:.4f} é™è‡³ {mae_3plus:.4f}',
                    'tradeoff': f'æ¨£æœ¬æ•¸æ¸›å°‘è‡³ {all_results["experiment_1_data_quality"]["min_raters_3"]["sample_count"]} ç­†'
                })

    # æª¢æŸ¥ XGBoost
    if 'experiment_2_xgboost' in all_results:
        if all_results['experiment_2_xgboost']['winner'] == 'xgboost':
            improvement = (all_results['experiment_2_xgboost']['random_forest']['mae'] -
                          all_results['experiment_2_xgboost']['xgboost']['mae'])
            recommendations.append({
                'priority': 'medium',
                'action': 'æ”¹ç”¨ XGBoost æ¨¡å‹',
                'reason': f'MAE æ”¹é€² {improvement:.4f} æ˜Ÿ',
                'tradeoff': 'éœ€è¦é¡å¤–å®‰è£ xgboost å¥—ä»¶'
            })

    # æª¢æŸ¥è¶…åƒæ•¸
    if 'experiment_3_hyperparameters' in all_results:
        exp3 = all_results['experiment_3_hyperparameters']
        if exp3['best'] != 'baseline':
            improvement = exp3['baseline']['mae'] - exp3[exp3['best']]['mae']
            if improvement > 0.01:  # æ”¹é€²è¶…é 0.01 æ˜Ÿ
                recommendations.append({
                    'priority': 'low',
                    'action': f'èª¿æ•´ RF è¶…åƒæ•¸ç‚º {exp3["best"]}',
                    'reason': f'MAE æ”¹é€² {improvement:.4f} æ˜Ÿ',
                    'tradeoff': 'è¼•å¾®å¢åŠ è¨“ç·´æ™‚é–“'
                })

    if recommendations:
        print("\næ¨è–¦æ”¹é€²æªæ–½ï¼ˆä¾å„ªå…ˆç´šæ’åºï¼‰ï¼š")
        for i, rec in enumerate(sorted(recommendations,
                                       key=lambda x: {'high': 1, 'medium': 2, 'low': 3}[x['priority']]), 1):
            priority_icon = {'high': 'ğŸ”´', 'medium': 'ğŸŸ¡', 'low': 'ğŸŸ¢'}[rec['priority']]
            print(f"\n{i}. {priority_icon} {rec['action']}")
            print(f"   ç†ç”±: {rec['reason']}")
            print(f"   ä»£åƒ¹: {rec['tradeoff']}")
    else:
        print("\nâœ… ç•¶å‰é…ç½®å·²ç¶“ç›¸ç•¶å„ªç§€ï¼Œç„¡æ˜é¡¯æ”¹é€²ç©ºé–“")

    # å„²å­˜çµæœ
    output_path = os.path.join(base_dir, 'results/improvement_experiments.json')
    with open(output_path, 'w', encoding='utf-8') as f:
        # Convert numpy types to Python types for JSON serialization
        def convert(obj):
            if isinstance(obj, np.integer):
                return int(obj)
            elif isinstance(obj, np.floating):
                return float(obj)
            elif isinstance(obj, np.ndarray):
                return obj.tolist()
            return obj

        json.dump(all_results, f, indent=2, ensure_ascii=False, default=convert)

    print(f"\nâœ… å¯¦é©—çµæœå·²å„²å­˜è‡³: {output_path}")


if __name__ == '__main__':
    main()
